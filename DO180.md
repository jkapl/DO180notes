# Overview of Container Technology
- Applications that rely on specific features or libraries in the OS (e.g. a particular version of a library like TLS 1.0) can break 
- container is a set of one or more processes that are isolated from the rest of the system. They also isolate the libraries and the runtime resources (such as CPU and storage) for an application to minimize the impact of any OS update to the host OS
    - When you build an application as a container image, which complies with the OCI standard, you can use any OCI-compliant container engine to execute the application.
    - Using containers, there is no longer a need to maintain separate production and development database servers. A single container image is used to create instances of the database service.\
    - many applications are not well suited for a containerized environment. For example, applications accessing low-level hardware information, such as memory, file systems, and devices may be unreliable due to container limitations.
- Kernel features that enable containers
    - Namespaces: resource isolation (processes, network iterfaces, mount points)
    - Control groups (cgroups): resource limits
    - Seccomp: limits how processes can use system calls
    - SELinux: Security Enhanced Linux, protects host from running processes and from each other
- Containers run an image: a file system bundle that contains dependencies required to execute a process
- Podman to manage containers
    - OCI Compliant and stores local images in local file-system so no need for daemon
***
## Podman
Useful commands:
- `podman search image-name`
- `podman run -d --name my-name actual-image-name echo "Hello!"`: -d detached mode, --name to give container custom name
- `podman inspect -l -f "{{.NetworkSettings.IPAddress}}"`: -l grabs latest used container, -f find
- `podman run -it image-name /bin/bash`: -it enable console interactivity
-  `podman run -e GREET=Hello -e NAME=RedHat rhel7"7/5 printenv GREET NAME`: -e set environment variables at container startup
**NOTE** Running a command after the container image name overrides the default command, but does not overwrite the default entrypoint. So with httpd for instance, the httpd service would not start if a command is passed in. Or with mysql, if a command is passed in, the mysql server will not start.
- Use -l flag to run a command in the last used container:
`sudo podman exec my-httpd-container cat /etc/hostname`
`sudo podman exec -l cat /etc/hostname`
- `podman inspect` to list metadata. Use Go template formatting to search with -f flag
`sudo podman inspect -f '{{ .NetworkSettings.IPAddress }}' my-httpd-container`
- `stop`, `kill`, `kill -s SIGKILL`
Stop then delete

### Persistent storage
Containers share read only image layers. Each container has access to its own read/write ephemeral layer. Podman can mount host directories inside a running container to provide persistent storage.
1. Create a director with owner and gropu root
`sudo mkdir /var/dbfiles/`
2. Give write permissions to the user id (UID) associated with the container. This UID may vary. In this example, it is 27
`sudo chown -R 27:27 /var/dbfiles`
3. Add the container_file_t SELinux context to the directory. This is necessary in RHEL for security.
`sudo semanage fcontext -a -t container_file_t '/var/dbfiles(/.*)?'`
4. Apply the SELinux container policy
`sudo restorecon -Rv /var/dbfiles `
Verify with
`ls -dZ /var/dbfiles`

To run the container with the file mount, use the `-v` flag.
`sudo podman run -v /var/dbfiles:/var/lib/mysql rhmap47/mysql`
This overlays the contents of /var/dbfiles onto the container directory /var/lib/mysql

### Networking
Podman attaches each container to a virtual bridge and assigns each container a private IP address
Config file is /etc/cni/net.d/87-podman-bridge.conflist
Traffic can flow to container with port forwarding rules. The following forwards traffic sent to port 8080 on the host to port 80 inside the container:
`sudo podman run -d --name apache1 -p 8080:80 rhscl/httpd-24-rhel7:2.4`

### Image Registries
Podman searches registries listed in this file: /etc/containers/registries.conf
`[registries.search]`
`registries = ["registry.access.redhat.com", "quay.io"]`

Save an image to a special .tar file using `podman save -o FILE_NAME IMAGE_NAME`: -o flag directs output to file instead of standard out. Tar file contains image metadata and original image layers. Load with `podman load -i FILE_NAME`
Delete images: `podman rmi`: -a flag for all, --force to remove all even if container is running
`podman commit [options] container-name [repository/image_name:tag]`: Create a new container image based on running container. -a for author
`podman diff container-name`: show which files were changed, created or deleted. 
**NOTE** Does not include mounted files, only container directories modified.
`podman inspect -f "{{range .Mounts}}{{println .Destination}}{{end}}" CONTAINER_NAME/ID`: list all mounted files and directories in running container
`podman tag image-name registry/name:tag`: tag an image

### Customizing images
- Red Hat Conatiner Catalog
- Quay.io (CoreOS)
- Docker Hub
- Source-to-Image (S2I)
    1. Start a container with a builder image which includes runtime, compilers, package managers
    2. Fetch application source
    3. Build application binary inside container
    4. Save container as a new container image including runtime and application binaries
#### Dockerfiles
ADD instruction copies files from a local or remote source. Decompresses the file if compressed
COPY copies files from working directory. Cannot copy a remote file, must use ADD
ENTRYPOINT specifies the default command to execute when image runs in a container. Default is /bin/sh -c
CMD provides default arguments for the ENTRYPOINT instruction. Will be overwritten if arguments follow the `podman run` command
`ENTRYPOINT ["/bin/date"]
CMD ["+%H:%M"]`
USER specifies username or UID to use when running the container with the RUN, CMD, and ENTRYPOINT instructions. It is a good practice to define a different user other than root for security reasons.
**NOTE** ADD and COPY copy files with same permissions and owner even if USER is specified. Use RUN instruction after the copy to modify permissions and owner to avoid errors.
Layering: Each command in Dockerfile creates new image layer. Use && operator to avoid large images.
`RUN yum --disablerepo=* --enablerepo="rhel-7-server-rpms" && yum update -y && yum install -y httpd`
Improve readability with \
```
RUN yum --disablerepo=* --enablerepo="rhel-7-server-rpms" && \
    yum update -y && \
    yum install -y httpd
```
`podman build -t --layers=false NAME:TAG DIR`: instructs podman to build the image, delete intermediate layers, assign name:tag and use dir as working directory
***
# Deploying Containerized Applications on OpenShift
- Controller: Kubernetes process that watches resources and makes changes attempting to move the current state towards the desired state
- Label: A key-value pair that can be assigned to any Kubernetes resource, used for filtering resouces

- Services: A single IP/port that provides access to a pool of pods
- Replication controller: defines how pods are replicated into different nodes. Usually created by deployment controller.
- ConfigMaps/Secrets: Configuration information. Secrets are always encoded and restricted to fewer authorized users.
Openshift resource types:
- Deployment config (dc): represents set of containers in a pod and deployment strategies used
- Build config (bc): defines a process to be executed and is used by S2I to build a container image from code in a Git repo. BC and DC together provide basoc CI/CD workflows
- Routes: Represent a DNS host name recognized by the OpenShift router as an ingress point
#### Networking
A route defines external-facing DNS names and ports for a service. A router (on OpenShift a pod running HAProxy) (ingress controller) forwards HTTP and TLS requests to the service addresses. Only requirement is that the DNS names are mapped to the IP addresses of the OpenShift router nodes.
There are two SDNs, Pod and Service
**Service discovery for pods to connect to services**
kubelet can specify DNS file with --resolv-conf flag! But not commonly used this way.
Handled by **kube-proxy** (which implements this either via iptables mode O(n) or IPVS mode O(1) (or user space proxy mode - first implementation.  This was slow because it copied packets, rewrote IP headers in userspace, sent them back to kernel)). **kube-proxy just a controller that communicates with the api server via HTTPS and rewrites ip tables accordingly.** Kube-proxy is a pod running on each node.
iptables mode: kube-proxy communicates with the kubernetes api server running on the master nodes. When services are added to the cluster, kube-proxy configures iptables on the worker nodes to correctly route traffic to service IPs. Kube-proxy writes iptables rules so that traffic heading to service IPs is redirected to the correct pod. [Destination NAT (DNAT) is used to make sure it gets to correct pod. conntrack is a Linux kernel connection tracker, and rewrites source IP when packet returns] Traffic is therefore handled at the kernel level by Linux netfilter. Typically here a CNI plugin is used. For instance, flannel will intercept traffic intended for service IPs and wrap those packets in
IPVS mode: similar to iptables mode. A load balancer living in the Linux kernel
user space proxy mode: kube proxy acts as a literal proxy. It configures ip tables rules to direct traffic to a port it opens on the node. kube proxy (listening at that port) then directs traffic to the backend pods in a round robin fashion. watches api server for updates on services and endpoints.
Two ways: environment variables or DNS
##### Environment variables:
- An application finds a service IP and port by using environment variables. When a pod is run on a node, the kubelet adds a set of environment variables for each active service. Kubernetes injects the following environment variables into containers for all pods:
SVC_NAME_SERVICE_HOST: service IP address
SVC_NAME_SERVICE_PORT: service TCP port
**NOTE**
When you have a Pod that needs to access a Service, and you are using the environment variable method to publish the port and cluster IP to the client Pods, you must create the Service before the client Pods come into existence. Otherwise, those client Pods won’t have their environment variables populated.
If you only use DNS to discover the cluster IP for a Service, you don’t need to worry about this ordering issue.
##### DNS:
If CoreDNS is running (OpenShift internal DNS server), each service is assigned an SRV record with an FQDN of the form:
SVC_NAME.PROJECT_NAME.svc.cluster.local
DNS just a service as well. In cluster DNS server runs as a pod and service. This allows applications to query by service name, and resolve service names to DNS records, which then produce IPs. Serves A and SRV records.
--
Access from outside the cluster is either from NodePort or route
Nodeport: older approach that exposes a port on each worker node and proxies those connections to the service IP
Routes: `oc expose`
Port forward: Can also foward ports on host to a pod. Does not use service and only forwards connections to a single pod. `oc port-forward`
##### Routes
Routes are implemented by a cluster-wide router service, which runs as a containerized application in the cluster. Router pods are scaled and replicated like any other OpenShift application.
Route connects public-facing IP address and DNS host name to an internal-facing service IP. Router pods bind to their nodes public IP addresses
Routes names are of the form:
route-name-project-name.default-domain
You can have multiple routes for the same service, provided they have different names.
#### Image streams
`oc new-app -i php http://services.lab.example.com/app --name=myapp`
oc new app creates
1. Image stream resouce
2. Build configuration resource
3. Deployment config resource
4. Service